{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62edc3bc",
   "metadata": {},
   "source": [
    "### Exercise 1: Hello World\n",
    "1. Write an MPI program which prints the message \"Hello World\"\n",
    "2. Modify your program so that each process prints out both its rank and the total number of processes P that the code is running on, i.e. the size of `MPI_COMM_WORLD`.\n",
    "3. Modify your program so that only a single controller process (e.g. rank 0) prints out a message (very useful when you run with hundreds of processes).\n",
    "4. What happens if you omit the final MPI procedure call in your program?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ebc605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part1\n",
    "from mpi4py import MPI\n",
    "COMM = MPI.COMM_WORLD\n",
    "RANK = COMM.Get_rank()\n",
    "SIZE = COMM.Get_size()\n",
    "print(\"hello\", RANK, SIZE)\n",
    "\n",
    "mpirun --oversubscribe -n 8 python3 ex1.py\n",
    "\n",
    "#part2\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "print(\"Process\", rank, \"of\", size, \"says Hello World! :)\")\n",
    "\n",
    "MPI.Finalize()\n",
    "\n",
    "#part3\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if rank == 0:\n",
    "    print(\"Hello World from controller process\")\n",
    "\n",
    "MPI.Finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452dca4e",
   "metadata": {},
   "source": [
    "### Exercise 2: Sharing Data\n",
    "Create a program that obtains an integer input from the terminal and distributes it to all the MPI processes.\n",
    "Each process must display its rank and the received value. \n",
    "Keep reading values until a negative integer is entered.\n",
    "**Output Example**\n",
    "```shell\n",
    "10\n",
    "Process 0 got 10\n",
    "Process 1 got 10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8b71b4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mpi4py'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmpi4py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MPI\n\u001b[0;32m      3\u001b[0m COMM \u001b[38;5;241m=\u001b[39mMPI\u001b[38;5;241m.\u001b[39mCOMM_WORLD\n\u001b[0;32m      4\u001b[0m RANK \u001b[38;5;241m=\u001b[39mCOMM\u001b[38;5;241m.\u001b[39mGet_rank()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mpi4py'"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "\n",
    "COMM =MPI.COMM_WORLD\n",
    "RANK =COMM.Get_rank()\n",
    "if RANK ==0:\n",
    "    sendbuf=int(input(\"donner n\"))\n",
    "else:\n",
    "    sendbuf =None\n",
    "recvbuf =COMM.bcast(sendbuf, root=0)\n",
    "print(RANK,recvbuf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cc6c82",
   "metadata": {},
   "source": [
    "### Exercise 3 Sending in a ring (broadcast by ring)\n",
    "\n",
    "Write a program that takes data from process zero and sends it to all of the other processes by sending it in a ring. That is, process i should receive the data add the rank of the process to it then send it to process i+1, until the last process is reached.\n",
    "Assume that the data consists of a single integer. Process zero reads the data from the user.\n",
    "print the process rank and the value received.\n",
    "\n",
    "\n",
    "![ring](../data/ring.gif)\n",
    "\n",
    "You may want to use these MPI routines in your solution:\n",
    "`Send` `Recv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc198e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "while(1):\n",
    "    if rank == 0:\n",
    "        x= int(input(\"entrer x\"))\n",
    "        comm.send(x, 1)\n",
    "    else:\n",
    "        x= comm.recv(source = rank - 1)\n",
    "        if rank < size - 1:\n",
    "            if x< 0: x-= rank\n",
    "            comm.send(x+ rank, rank + 1)\n",
    "    if x< 0:\n",
    "        break\n",
    "    print(\"rank:\", rank, \",data:\",x)\n",
    "MPI.Finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02855dd2",
   "metadata": {},
   "source": [
    "### Exercise 4: Scattering Matrix\n",
    "1. Create an n by m matrix A on processor 0.\n",
    "2. Use MPI_Scatterv to send parts of the matrix to the other processors.\n",
    "3. Processor 1 receives A(i,j) for i=0 to (n/2)-1 and j=m/2 to m-1.\n",
    "4. Processor 2 receives A(i,j) for i=n/2 to n-1 and j=0 to (m/2)-1.\n",
    "5. Processor 3 receives A(i,j) for i=n/2 to n-1 and j=m/2 to m-1.\n",
    "**Example:** using n=m=8 for simplicity.\n",
    "\n",
    "![N2utM.png](attachment:N2utM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340a61ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "n = 8\n",
    "m = 8\n",
    "if rank == 0:\n",
    "    A = np.random.rand(n,m)\n",
    "    print(\"Original matrix on processor 0:\")\n",
    "    print(A)\n",
    "    # Divide the matrix into parts to send to each processor\n",
    "    sendcounts = np.zeros(size, dtype=int)\n",
    "    displs = np.zeros(size, dtype=int)\n",
    "    sendcounts[1] = (n // 2) * (m - m // 2)\n",
    "    sendcounts[2] = (n - n // 2) * (m // 2)\n",
    "    sendcounts[3] = (n - n // 2) * (m - m // 2)\n",
    "    displs[1] = (n // 2) * m + m // 2\n",
    "    displs[2] = n // 2\n",
    "    displs[3] = (n // 2) * m + m // 2 + n // 2\n",
    "else:\n",
    "    A = None\n",
    "    sendcounts = None\n",
    "    displs = None\n",
    "# Scatter the matrix parts to each processor\n",
    "recvA = np.zeros((n // 2, m // 2))\n",
    "recvcounts = (n // 2) * (m // 2)\n",
    "comm.Scatterv([A, sendcounts, displs, MPI.DOUBLE], recvA, root=0)\n",
    "if rank == 1:\n",
    "    print(\"Received matrix on processor 1:\")\n",
    "    print(recvA)\n",
    "elif rank == 2:\n",
    "    print(\"Received matrix on processor 2:\")\n",
    "    print(recvA)\n",
    "elif rank == 3:\n",
    "    print(\"Received matrix on processor 3:\")\n",
    "    print(recvA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fde7aa",
   "metadata": {},
   "source": [
    "### Exercise 5 Matrix vector product\n",
    "\n",
    "1. Use the `MatrixVectorMult.py` file to implement the MPI version of matrix vector multiplication.\n",
    "2. Process 0 compares the result with the `dot` product.\n",
    "3. Plot the scalability of your implementation. \n",
    "\n",
    "**Output Example**\n",
    "```shell\n",
    "CPU time of parallel multiplication using 2 processes is  174.923446\n",
    "The error comparing to the dot product is : 1.4210854715202004e-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2103857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "from numpy.random import rand, seed\n",
    "from numba import njit\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "seed(42)\n",
    "@njit\n",
    "def matrix_vector_mult(A, b, x):\n",
    "    row, col = A.shape\n",
    "    for i in range(row):\n",
    "        a = A[i]\n",
    "        for j in range(col):\n",
    "            x[i] += a[j] * b[j]\n",
    "\n",
    "    return x\n",
    "\n",
    "matrix_size = 1000\n",
    "block_size = matrix_size // size\n",
    "\n",
    "if rank == 0:\n",
    "    A = lil_matrix((matrix_size, matrix_size))\n",
    "    A[0, :100] = rand(100)\n",
    "    A[1, 100:200] = A[0, :100]\n",
    "    A.setdiag(rand(matrix_size))\n",
    "    A = A.toarray()\n",
    "    b = rand(matrix_size)\n",
    "else:\n",
    "    A = None\n",
    "    b = None\n",
    "\n",
    "matrix = np.zeros((block_size, matrix_size))\n",
    "comm.Scatter(A, matrix, root=0)\n",
    "\n",
    "b = comm.bcast(b, root=0)\n",
    "\n",
    "block_result = np.zeros(block_size)\n",
    "\n",
    "start_time = MPI.Wtime()\n",
    "matrix_vector_mult(matrix, b, block_result)\n",
    "stop_time = MPI.Wtime()\n",
    "\n",
    "send_counts = np.array(comm.gather(len(block_result), root=0))\n",
    "\n",
    "if rank == 0:\n",
    "    result = np.zeros(sum(send_counts), dtype=np.double)\n",
    "else:\n",
    "    result = None\n",
    "\n",
    "comm.Gatherv(block_result, recvbuf=(result, send_counts, MPI.DOUBLE), root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    dot_product_result = A.dot(b)\n",
    "    print(\"CPU time of matrix multiplication is \", (stop_time - start_time) * 1000)\n",
    "    print(\"The error comparing to the dot product is:\", np.max(np.abs(dot_product_result - result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4752f9",
   "metadata": {},
   "source": [
    "### Exercise 6: Pi calculation\n",
    "An approximation to the value π can be obtained from the following expression\n",
    "\n",
    "$$\n",
    "\\frac{\\pi}{4}=\\int_0^1 \\frac{d x}{1+x^2} \\approx \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{1+\\left(\\frac{i-\\frac{1}{2}}{N}\\right)^2}\n",
    "$$\n",
    "\n",
    "where the answer becomes more accurate with increasing N. Iterations over i are independent so the\n",
    "calculation can be parallelized.\n",
    "\n",
    "For the following exercises you should set N = 840. This number is divisible by 2, 3, 4, 5, 6, 7 and 8\n",
    "which is convenient when you parallelize the calculation!\n",
    "\n",
    "1. Create a program where each process independently computes the value of `π` and prints it to the screen. Check that the values are correct (each process should print the same value)\n",
    "2. Now arrange for different processes to do the computation for different ranges of i. For example, on two processes: rank 0 would do i = 0, 1, 2, . . . , N/2 - 1; rank 1 would do i = N/2, N/2 + 1, . . . , N-1.\n",
    "Print the partial sums to the screen and check the values are correct by adding them up by hand.\n",
    "3. Now we want to accumulate these partial sums by sending them to the controller (e.g. rank 0) to add up:\n",
    "- all processes (except the controller) send their partial sum to the controller\n",
    "- the controller receives the values from all the other processes, adding them to its own partial sum\n",
    "1. Use the function `MPI_Wtime` (see below) to record the time it takes to perform the calculation. For a given value of N, does the time decrease as you increase the number of processes? Note that to ensure that the calculation takes a sensible amount of time (e.g. more than a second) you will probably have to perform the calculation of `π` several thousands of times.\n",
    "2. Ensure your program works correctly if N is not an exact multiple of the number of processes P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part1\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "N = 840\n",
    "dx = 1.0/N\n",
    "\n",
    "# Compute the partial sum for each process\n",
    "start = rank*N//size + 1\n",
    "end = (rank+1)*N//size\n",
    "partial_sum = np.sum(1.0/(1.0 + ((np.arange(start, end) - 0.5)/N)**2))\n",
    "\n",
    "# Reduce the partial sums to compute the final result\n",
    "pi = comm.reduce(partial_sum, op=MPI.SUM, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    pi = 4.0*dx*pi\n",
    "    print(f\"Final result: {pi:.10f}\")\n",
    "    \n",
    "#part2\n",
    "from mpi4py import MPI\n",
    "import math\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "N = 840\n",
    "start_i = int(N/2)*rank\n",
    "end_i = int(N/2)*(rank+1)\n",
    "\n",
    "pi_part = 0.0\n",
    "for i in range(start_i, end_i):\n",
    "    pi_part += 1.0/(1.0 + ((i + 0.5)/N)**2)\n",
    "pi_part *= 4.0/N\n",
    "\n",
    "print(\"Process\", rank, \"partial sum:\", pi_part)\n",
    "\n",
    "if rank == 0:\n",
    "    pi = pi_part + comm.recv(source=1)\n",
    "    print(\"Computed pi:\", pi)\n",
    "else:\n",
    "    comm.send(pi_part, dest=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
